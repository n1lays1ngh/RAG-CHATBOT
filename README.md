# ğŸ§  RAG Chatbot with Local LLM (FastAPI + React + Ollama)

A full-stack chatbot application that uses **Retrieval-Augmented Generation (RAG)** powered by a **local LLM (Phi-3 via Ollama)**. Upload documents (PDF, TXT, etc.), ask questions, and get context-aware responses â€” all running locally with **FastAPI**, **FAISS**, **React**, and **Tailwind CSS**.

---

## ğŸš€ Features

- ğŸ“„ **File Upload**: Ingest your own documents (PDF, TXT, etc.)
- ğŸ§  **Local LLM Support**: Chat powered by Phi-3 via Ollama (runs locally)
- ğŸ“š **FAISS + RAG**: Retrieves relevant chunks from your documents
- ğŸ—¨ï¸ **Interactive Chat UI**: Real-time chat using React
- âš¡ **FastAPI Backend**: Lightweight and fast REST API
- ğŸ¨ **Tailwind CSS**: Clean and responsive UI
- ğŸ”Œ **Streaming Chat Support** (optional toggle)

---

## ğŸ§± Tech Stack

| Layer     | Tech                                  |
|-----------|---------------------------------------|
| Frontend  | React, Tailwind CSS, Vite             |
| Backend   | FastAPI, Python                       |
| RAG       | FAISS, SentenceTransformers           |
| LLM       | [Ollama](https://ollama.com/) + Phi-3 |
| Others    | Axios, Langchain-style logic (custom) |

---

## ğŸ“‚ Project Structure

```
RAG-CHATBOT-main/
â”œâ”€â”€ backend/            # FastAPI backend
â”‚   â”œâ”€â”€ chat.py         # Chat endpoint
â”‚   â”œâ”€â”€ ingest.py       # File ingestion & indexing
â”‚   â”œâ”€â”€ main.py         # App entrypoint
â”‚   â”œâ”€â”€ utils/          # Embedding, retrieval, Ollama
â”‚   â””â”€â”€ vector_store/   # FAISS index storage
â”œâ”€â”€ frontend/           # React + Tailwind app
â”‚   â”œâ”€â”€ src/components/ # ChatBox, FileUpload
â”‚   â””â”€â”€ api.js          # Axios API config
```

---

## âš™ï¸ Setup Instructions

### 1. Prerequisites

- Python 3.10+
- Node.js 16+
- Ollama installed and running locally with Phi-3 model:

```bash
ollama run phi3
```

---

### 2. Backend (FastAPI)

```bash
cd backend
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
uvicorn main:app --reload
```

- Ingest documents at: `POST /ingest`
- Chat endpoint: `POST /chat`

---

### 3. Frontend (React + Vite)

```bash
cd frontend
npm install
npm run dev
```

- Open your browser at `http://localhost:5173/`

---

## ğŸ“¡ API Endpoints

### `POST /ingest`
Uploads and indexes documents.
```json
{
  "file": "<your_file.pdf|txt>"
}
```

### `POST /chat`
Sends a user query to the LLM with optional chat history.
```json
{
  "query": "What is the document about?",
  "history": [["Hello", "Hi there!"]]
}
```

---

## ğŸ’¬ Chat UI

- File Upload â†’ Indexes content via FastAPI.
- ChatBox â†’ Sends query to local Phi-3 and gets RAG-augmented responses.
- Uses Axios for API calls.

---

## ğŸ§ª Development Tips

- You can test APIs with Postman or directly via the frontend.
- Modify vector store settings or top-k retrievals in `retriever.py`.
- Ollama should be running with your desired model (`phi3`, `llama3`, etc.).

---

## ğŸ“ License

MIT License

---

## ğŸ™‹â€â™‚ï¸ Author

Built by **Nilay Singh**  
[ğŸ”— LinkedIn](https://www.linkedin.com) | [ğŸ™ GitHub](https://github.com)

---

## ğŸ“¸ Screenshots (optional)

> *(Add screenshots of your UI here once ready)*